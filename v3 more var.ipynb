{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.2)\n",
      "Requirement already satisfied: pyfume in /opt/conda/lib/python3.10/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: simpful in /opt/conda/lib/python3.10/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.10.0)\n",
      "Requirement already satisfied: fst-pso in /opt/conda/lib/python3.10/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in /opt/conda/lib/python3.10/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Requirement already satisfied: pyrsm in /home/jovyan/.rsm-msba/lib/python3.10/site-packages (0.7.5)\n",
      "Requirement already satisfied: ipynbname>=2021.3.2 in /opt/conda/lib/python3.10/site-packages (from pyrsm) (2021.3.2)\n",
      "Requirement already satisfied: matplotlib>=3.1.1 in /opt/conda/lib/python3.10/site-packages (from pyrsm) (3.6.2)\n",
      "Requirement already satisfied: statsmodels>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from pyrsm) (0.13.5)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from pyrsm) (0.12.2)\n",
      "Requirement already satisfied: IPython>=8.0.1 in /opt/conda/lib/python3.10/site-packages (from pyrsm) (8.8.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from pyrsm) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from pyrsm) (1.2.0)\n",
      "Requirement already satisfied: pandas>=0.25.2 in /opt/conda/lib/python3.10/site-packages (from pyrsm) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from pyrsm) (1.10.0)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from ipynbname>=2021.3.2->pyrsm) (6.19.4)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (0.1.6)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (5.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (0.18.2)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (0.6.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (2.14.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from IPython>=8.0.1->pyrsm) (3.0.36)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->pyrsm) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->pyrsm) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->pyrsm) (9.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->pyrsm) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->pyrsm) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->pyrsm) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->pyrsm) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.1->pyrsm) (22.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.2->pyrsm) (2022.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pyrsm) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pyrsm) (3.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.10.1->pyrsm) (0.5.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->IPython>=8.0.1->pyrsm) (0.8.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels>=0.10.1->pyrsm) (1.16.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->IPython>=8.0.1->pyrsm) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.11->IPython>=8.0.1->pyrsm) (0.2.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->ipynbname>=2021.3.2->pyrsm) (5.9.4)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->ipynbname>=2021.3.2->pyrsm) (7.4.8)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->ipynbname>=2021.3.2->pyrsm) (6.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->ipynbname>=2021.3.2->pyrsm) (1.6.5)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->ipynbname>=2021.3.2->pyrsm) (1.5.6)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.10/site-packages (from ipykernel->ipynbname>=2021.3.2->pyrsm) (24.0.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->ipynbname>=2021.3.2->pyrsm) (0.1.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->IPython>=8.0.1->pyrsm) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->IPython>=8.0.1->pyrsm) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->IPython>=8.0.1->pyrsm) (0.2.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->ipynbname>=2021.3.2->pyrsm) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->ipynbname>=2021.3.2->pyrsm) (5.1.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->ipynbname>=2021.3.2->pyrsm) (2.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install pyrsm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from gensim.models import TfidfModel\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as sp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to build a vocabulary based on descending word frequencies \n",
    "def build_vocab(sentences):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to learn word embeddings through Word2vec module\n",
    "def get_embeddings(inp_data, vocabulary_inv, size_features=100,\n",
    "                   mode='skipgram',\n",
    "                   min_word_count=2,\n",
    "                   context=5):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 15  # Number of threads to run in parallel\n",
    "    downsampling = 1e-3  # Downsample setting for frequent words\n",
    "    print('Training Word2Vec model...')\n",
    "    # use inp_data and vocabulary_inv to reconstruct sentences\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == 'skipgram':\n",
    "        sg = 1\n",
    "        print('Model: skip-gram')\n",
    "    elif mode == 'cbow':\n",
    "        sg = 0\n",
    "        print('Model: CBOW')\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                        sg=sg,\n",
    "                                        vector_size=size_features,\n",
    "                                        min_count=min_word_count,\n",
    "                                        window=context,\n",
    "                                        sample=downsampling)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model.wv:\n",
    "            embedding_weights[i] = embedding_model.wv[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "                                                     embedding_model.vector_size)\n",
    "    return embedding_weights, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "def preprocess_df(df, stemming=False):\n",
    "    # get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('would')\n",
    "    stop_words.add('The')\n",
    "    # prepare translation table to translate punctuation to space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    preprocessed_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = row[\"text\"]\n",
    "        sent_nopuncts = sent.translate(translator)\n",
    "        words_list = sent_nopuncts.strip().split()\n",
    "        if stemming == True:\n",
    "            words_list = [ps.stem(word) for word in words_list]\n",
    "        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1] # also skip space from above translation\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "    df[\"text\"] = preprocessed_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attributes(df, test=False):\n",
    "    attribute_list = ['name','stars','review_count','attributes.OutdoorSeating','attributes.BusinessAcceptsCreditCards','attributes.RestaurantsTableService',\n",
    "    'attributes.RestaurantsReservations','attributes.RestaurantsPriceRange2','attributes.HasTV','attributes.RestaurantsGoodForGroups','attributes.Caters',\n",
    "    'attributes.RestaurantsTakeOut','attributes.RestaurantsDelivery','attributes.GoodForKids', 'attributes.BikeParking', 'latitude', 'longitude', 'postal_code', 'is_open', 'text']\n",
    "    if test == False:\n",
    "        attribute_list.append('label')\n",
    "    else:\n",
    "        pass\n",
    "    df = df[attribute_list]\n",
    "    col_names = df.columns\n",
    "    col_name_clean = list(df[attribute_list].columns.str.replace('attributes.', ''))\n",
    "    new_column_names = {column: column.replace('attributes.', '') for column in col_names}\n",
    "    df = df.rename(columns=new_column_names)\n",
    "    d = {'False': 0, 'True': 1}\n",
    "    pattern = r'\\b\\d+\\b(?:\\s+\\b\\d+\\b)*\\s*'\n",
    "    #clean b'\n",
    "    for col in df:\n",
    "        if col != 'text' and col != 'label' and col != 'name' and col != 'latitude' and col != 'longitude' and col != 'postal_code' and col != 'is_open':\n",
    "            try:\n",
    "                df[col] = df[col].str.extract(r\"b'(.*?)'\")\n",
    "                df[col] = df[col].map(d)\n",
    "            except:\n",
    "                pass\n",
    "        if col == 'name' or col == 'postal_code':\n",
    "            df[col] = df[col].str.extract(r\"b'(.*?)'\")\n",
    "        ## 0 if Canada postal code 1 if US\n",
    "        if col == 'postal_code':\n",
    "            df[col] = df[col].str.match(pattern)\n",
    "    df = df.fillna(0)\n",
    "    df['text'] = df['name'].astype(str) + ' ' + df['text']\n",
    "    df[['OutdoorSeating','BusinessAcceptsCreditCards','RestaurantsTableService','RestaurantsReservations','RestaurantsPriceRange2','HasTV','RestaurantsGoodForGroups',\n",
    "    'Caters','RestaurantsTakeOut','RestaurantsDelivery','GoodForKids', 'postal_code', 'is_open', 'BikeParking']] = df[['OutdoorSeating','BusinessAcceptsCreditCards','RestaurantsTableService',\n",
    "    'RestaurantsReservations','RestaurantsPriceRange2','HasTV','RestaurantsGoodForGroups','Caters','RestaurantsTakeOut','RestaurantsDelivery','GoodForKids', 'postal_code', 'is_open', 'BikeParking']].astype('category')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_vali_test(X, y, ratio_train = 0.8, ratio_val = 0.1, ratio_test = 0.1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ratio_test, random_state=42, shuffle=True)\n",
    "    ratio_remaining = 1 - ratio_test\n",
    "    ratio_val_adjusted = ratio_val / ratio_remaining\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=ratio_val_adjusted, random_state=42, shuffle=True)\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1069/1101805200.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  col_name_clean = list(df[attribute_list].columns.str.replace('attributes.', ''))\n",
      "/tmp/ipykernel_1069/1101805200.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  col_name_clean = list(df[attribute_list].columns.str.replace('attributes.', ''))\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/.\"\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "df_train[\"text\"] = df_train[\"review\"]\n",
    "df_test[\"text\"] = df_test[\"review\"]\n",
    "df_train = preprocess_df(df_train, stemming=False)\n",
    "df_test = preprocess_df(df_test, stemming=False)\n",
    "df_train = process_attributes(df_train)\n",
    "df_test = process_attributes(df_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf, X_test_rf, X_val_rf, y_train_rf, y_test_rf, y_val_rf = \\\n",
    "    get_train_vali_test(df_train.drop(['text', 'label', 'name', 'RestaurantsPriceRange2'], axis=1), df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3726235741444867"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(max_depth=11, n_estimators=500, max_features=3, random_state=0)\n",
    "clf_rf.fit(X_train_rf, y_train_rf)\n",
    "f1_score(y_test_rf, clf_rf .predict(X_test_rf), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4038022813688213"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_rf, clf_rf .predict(X_val_rf), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7785755177018053"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test_rf, clf_rf.predict_proba(X_test_rf), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X_train_rf.columns)\n",
    "importance = pd.Series(clf_rf.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "feature_imp = pd.DataFrame(sorted(zip(importance,importance.index)), columns=['Value','Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "latitude                      0.164870\n",
       "longitude                     0.164245\n",
       "review_count                  0.129298\n",
       "stars                         0.086543\n",
       "RestaurantsDelivery           0.063767\n",
       "OutdoorSeating                0.047138\n",
       "RestaurantsReservations       0.045378\n",
       "GoodForKids                   0.043876\n",
       "HasTV                         0.038531\n",
       "Caters                        0.033661\n",
       "is_open                       0.030708\n",
       "RestaurantsTableService       0.029000\n",
       "BikeParking                   0.028762\n",
       "postal_code                   0.027794\n",
       "RestaurantsGoodForGroups      0.022933\n",
       "BusinessAcceptsCreditCards    0.022625\n",
       "RestaurantsTakeOut            0.020870\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=True,\n",
    "                        preprocessor=None,  # applied preprocessor in Data Cleaning\n",
    "                        tokenizer=word_tokenize,\n",
    "                        use_idf=True,\n",
    "                        norm='l2',\n",
    "                        smooth_idf=True,\n",
    "                        stop_words= 'english',\n",
    "                        max_df=0.4,\n",
    "                        sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Rush Inn So stopped way Side Quest street nWe ...\n",
       "1        GreenMix This go healthy spot food always fres...\n",
       "2        BarBurrito - Gerrard Food court meal Gerrard S...\n",
       "3        SalvaMex Located Rainbow Charleston small fami...\n",
       "4        Hop Hing No frills Chinese takeout joint serve...\n",
       "                               ...                        \n",
       "13139    Mariscos Vuelve a La Vida Worst food ever asha...\n",
       "13140    Alize Catering My experience restaurant little...\n",
       "13141    Taco Bell Giving one star write review Place s...\n",
       "13142    Sushi Kai It good deal voucher However food no...\n",
       "13143    China House write many reviews place good food...\n",
       "Name: text, Length: 13144, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X =  tfidf.fit_transform(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = get_train_vali_test(X, df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_vec =  tfidf.fit_transform(X_train[\"text\"])\n",
    "#additional_features = sp.csr_matrix(X_train.drop(['text'], axis=1))\n",
    "#X_train = sp.hstack((tfidf_vec,additional_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8167300380228137"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=100000000, C=5, class_weight='balanced').fit(X_train, y_train)\n",
    "f1_score(y_test, clf.predict(X_test), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8076045627376426"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, clf.predict(X_val), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9686476545898808"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, clf.predict_proba(X_test), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.969639674526927"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, 0.9*clf.predict_proba(X_test) + 0.1*clf_rf.predict_proba(X_test_rf), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['american (new)', 'american (traditional)', 'asian fusion',\n",
       "       'canadian (new)', 'chinese', 'italian', 'japanese',\n",
       "       'mediterranean', 'mexican', 'thai'], dtype=object)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8212927756653993"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_index = [d.tolist().index(max(d)) for d in (0.6*clf.predict_proba(X_test) + 0.4*clf_rf.predict_proba(X_test_rf))]\n",
    "pred_test = [clf.classes_[i] for i in pred_index]\n",
    "f1_score(y_test, pred_test, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.817490494296578"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_index = [d.tolist().index(max(d)) for d in (0.6*clf.predict_proba(X_val) + 0.4*clf_rf.predict_proba(X_val_rf))]\n",
    "pred_val = [clf.classes_[i] for i in pred_index]\n",
    "f1_score(y_val, pred_val, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.73368607, 0.51515152, 0.51764706, 0.93841642,\n",
       "       0.93203883, 0.91089109, 0.88      , 0.94444444, 0.92631579])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, pred_val, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70,  61,   2,   3,   1,   4,   0,   5,   3,   0],\n",
       "       [ 41, 208,   0,   9,   1,   4,   0,   0,   2,   0],\n",
       "       [  2,   2,  17,   2,   9,   1,   4,   0,   1,   2],\n",
       "       [  2,   9,   0,  22,   0,   2,   0,   0,   2,   0],\n",
       "       [  1,   2,   0,   1, 160,   0,   2,   0,   0,   1],\n",
       "       [  5,   7,   0,   5,   0, 192,   0,   0,   0,   0],\n",
       "       [  3,   2,   3,   0,   3,   0,  92,   0,   1,   0],\n",
       "       [  1,   4,   0,   6,   0,   0,   0,  66,   2,   0],\n",
       "       [  6,   6,   1,   0,   0,   0,   0,   0, 204,   0],\n",
       "       [  0,   1,   3,   0,   0,   0,   0,   0,   0,  44]], dtype=int64)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, pred_val, labels=clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in your implemetation, create the output file using the same format\n",
    "dic = {\"Id\": [], \"Predicted\": []}\n",
    "df_X_test_lr = tfidf.transform(df_test['text'])\n",
    "df_X_test_rf = df_test.drop(['text', 'name', 'RestaurantsPriceRange2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = [d.tolist().index(max(d)) for d in (0.9*clf.predict_proba(df_X_test_lr) + 0.1*clf_rf.predict_proba(df_X_test_rf))]\n",
    "preds = [clf.classes_[i] for i in pred_index]\n",
    "for i, pred in enumerate(preds):\n",
    "    dic[\"Id\"].append(i)\n",
    "    dic[\"Predicted\"].append(pred)\n",
    "\n",
    "dic_df = pd.DataFrame.from_dict(dic)\n",
    "#dic_df.to_csv(\"predicted.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "51ff2e06048c899726fdf58076269735b99ca5d50955928d5afc64b7e9d33ce9"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
