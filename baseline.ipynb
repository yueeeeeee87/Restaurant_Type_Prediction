{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.1 MB)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Using cached FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (5.2.1)\n",
      "Collecting pyfume\n",
      "  Using cached pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Collecting fst-pso\n",
      "  Using cached fst_pso-1.8.1-py3-none-any.whl\n",
      "Collecting simpful\n",
      "  Using cached simpful-2.9.0-py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful\n",
      "  Using cached miniful-0.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.13)\n",
      "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM, gensim\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyldavis 3.3.1 requires sklearn, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 gensim-4.3.0 miniful-0.0.6 pyfume-0.2.25 simpful-2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to build a vocabulary based on descending word frequencies \n",
    "def build_vocab(sentences):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to learn word embeddings through Word2vec module\n",
    "def get_embeddings(inp_data, vocabulary_inv, size_features=100,\n",
    "                   mode='skipgram',\n",
    "                   min_word_count=2,\n",
    "                   context=5):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 15  # Number of threads to run in parallel\n",
    "    downsampling = 1e-3  # Downsample setting for frequent words\n",
    "    print('Training Word2Vec model...')\n",
    "    # use inp_data and vocabulary_inv to reconstruct sentences\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == 'skipgram':\n",
    "        sg = 1\n",
    "        print('Model: skip-gram')\n",
    "    elif mode == 'cbow':\n",
    "        sg = 0\n",
    "        print('Model: CBOW')\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                        sg=sg,\n",
    "                                        vector_size=size_features,\n",
    "                                        min_count=min_word_count,\n",
    "                                        window=context,\n",
    "                                        sample=downsampling)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model.wv:\n",
    "            embedding_weights[i] = embedding_model.wv[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "                                                     embedding_model.vector_size)\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    # get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('would')\n",
    "    # prepare translation table to translate punctuation to space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    preprocessed_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = row[\"text\"]\n",
    "        sent_nopuncts = sent.translate(translator)\n",
    "        words_list = sent_nopuncts.strip().split()\n",
    "        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1] # also skip space from above translation\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "    df[\"text\"] = preprocessed_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/.\"\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "df_train[\"text\"] = df_train[\"review\"]\n",
    "df_test[\"text\"] = df_test[\"review\"]\n",
    "df_train = preprocess_df(df_train)\n",
    "df_test = preprocess_df(df_test)\n",
    "\n",
    "# tokenization \n",
    "tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "# build vocabulary from tokenized data\n",
    "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
    "# use the above mapping to create input data\n",
    "inp_data = [[vocabulary[word] for word in text] for text in tagged_data]\n",
    "# get embedding vector\n",
    "embedding_weights = get_embeddings(inp_data, vocabulary_inv)\n",
    "\n",
    "\n",
    "tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test[\"text\"])]\n",
    "\n",
    "train_vec = []\n",
    "for doc in tagged_train_data:\n",
    "    vec = 0\n",
    "    for w in doc:\n",
    "        vec += embedding_weights[vocabulary[w]]\n",
    "    vec = vec / len(doc)\n",
    "    train_vec.append(vec)\n",
    "\n",
    "test_vec = []\n",
    "for doc in tagged_test_data:\n",
    "    vec = 0\n",
    "    length = 0\n",
    "    for w in doc:\n",
    "        try:\n",
    "            vec += embedding_weights[vocabulary[w]]\n",
    "            length += 1\n",
    "        except:\n",
    "            continue\n",
    "    vec = vec / length\n",
    "    test_vec.append(vec)\n",
    "\n",
    "clf = LogisticRegression(max_iter=100000000).fit(train_vec, df_train[\"label\"])\n",
    "preds = clf.predict(test_vec)\n",
    "\n",
    "# in your implemetation, create the output file using the same format\n",
    "dic = {\"Id\": [], \"Predicted\": []}\n",
    "for i, pred in enumerate(preds):\n",
    "    dic[\"Id\"].append(i)\n",
    "    dic[\"Predicted\"].append(pred)\n",
    "\n",
    "dic_df = pd.DataFrame.from_dict(dic)\n",
    "dic_df.to_csv(\"predicted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>attributes.HappyHour</th>\n",
       "      <th>attributes.Ambience</th>\n",
       "      <th>hours.Tuesday</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>attributes.AgesAllowed</th>\n",
       "      <th>attributes.GoodForDancing</th>\n",
       "      <th>attributes.OutdoorSeating</th>\n",
       "      <th>hours.Saturday</th>\n",
       "      <th>attributes.Corkage</th>\n",
       "      <th>...</th>\n",
       "      <th>attributes.RestaurantsDelivery</th>\n",
       "      <th>attributes.DietaryRestrictions</th>\n",
       "      <th>attributes.BusinessAcceptsBitcoin</th>\n",
       "      <th>address</th>\n",
       "      <th>attributes.GoodForKids</th>\n",
       "      <th>attributes.GoodForMeal</th>\n",
       "      <th>hours</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>b\"{'romantic': False, 'intimate': False, 'clas...</td>\n",
       "      <td>b'15:0-2:0'</td>\n",
       "      <td>b'44107'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b'11:30-2:0'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'17800 Detroit Ave'</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b\"{'dessert': False, 'latenight': False, 'lunc...</td>\n",
       "      <td>{'Monday': '16:0-2:0', 'Tuesday': '15:0-2:0', ...</td>\n",
       "      <td>american (traditional)</td>\n",
       "      <td>So, we stopped here on our way to the Side Que...</td>\n",
       "      <td>So stopped way Side Quest street nWe know expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b\"{'romantic': False, 'intimate': False, 'tour...</td>\n",
       "      <td>b'11:0-21:0'</td>\n",
       "      <td>b'85042'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>b'11:0-20:30'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>b'2160 E Baseline Rd, Ste 128'</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>b\"{'dessert': False, 'latenight': False, 'lunc...</td>\n",
       "      <td>{'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...</td>\n",
       "      <td>american (new)</td>\n",
       "      <td>This is our go-to healthy spot! The food is al...</td>\n",
       "      <td>This go healthy spot The food always fresh del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'11:0-21:0'</td>\n",
       "      <td>b'M4M 3G6'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'11:0-21:0'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'1000 Gerrard St E'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...</td>\n",
       "      <td>mexican</td>\n",
       "      <td>Food court meal at Gerrard Square.  It's been ...</td>\n",
       "      <td>Food court meal Gerrard Square It since nachos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b\"{'romantic': False, 'intimate': False, 'clas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'89146'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'6700 W Charleston Blvd'</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mexican</td>\n",
       "      <td>Located on Rainbow/Charleston, this small fami...</td>\n",
       "      <td>Located Rainbow Charleston small family owned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b\"{'romantic': False, 'intimate': False, 'tour...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'44133'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>b'False'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'5630 Wallings Rd'</td>\n",
       "      <td>b'True'</td>\n",
       "      <td>b\"{'dessert': False, 'latenight': False, 'lunc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chinese</td>\n",
       "      <td>No frills Chinese takeout joint which serves u...</td>\n",
       "      <td>No frills Chinese takeout joint serves best ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id attributes.HappyHour                                attributes.Ambience  \\\n",
       "0   0              b'True'  b\"{'romantic': False, 'intimate': False, 'clas...   \n",
       "1   1                  NaN  b\"{'romantic': False, 'intimate': False, 'tour...   \n",
       "2   2                  NaN                                                NaN   \n",
       "3   3                  NaN  b\"{'romantic': False, 'intimate': False, 'clas...   \n",
       "4   4                  NaN  b\"{'romantic': False, 'intimate': False, 'tour...   \n",
       "\n",
       "  hours.Tuesday postal_code attributes.AgesAllowed attributes.GoodForDancing  \\\n",
       "0   b'15:0-2:0'    b'44107'                    NaN                       NaN   \n",
       "1  b'11:0-21:0'    b'85042'                    NaN                       NaN   \n",
       "2  b'11:0-21:0'  b'M4M 3G6'                    NaN                       NaN   \n",
       "3           NaN    b'89146'                    NaN                       NaN   \n",
       "4           NaN    b'44133'                    NaN                       NaN   \n",
       "\n",
       "  attributes.OutdoorSeating hours.Saturday attributes.Corkage  ...  \\\n",
       "0                  b'False'   b'11:30-2:0'                NaN  ...   \n",
       "1                   b'True'  b'11:0-20:30'                NaN  ...   \n",
       "2                       NaN   b'11:0-21:0'                NaN  ...   \n",
       "3                  b'False'            NaN                NaN  ...   \n",
       "4                  b'False'            NaN                NaN  ...   \n",
       "\n",
       "   attributes.RestaurantsDelivery attributes.DietaryRestrictions  \\\n",
       "0                        b'False'                            NaN   \n",
       "1                        b'False'                            NaN   \n",
       "2                             NaN                            NaN   \n",
       "3                         b'True'                            NaN   \n",
       "4                        b'False'                            NaN   \n",
       "\n",
       "  attributes.BusinessAcceptsBitcoin                         address  \\\n",
       "0                               NaN            b'17800 Detroit Ave'   \n",
       "1                          b'False'  b'2160 E Baseline Rd, Ste 128'   \n",
       "2                               NaN            b'1000 Gerrard St E'   \n",
       "3                               NaN       b'6700 W Charleston Blvd'   \n",
       "4                               NaN             b'5630 Wallings Rd'   \n",
       "\n",
       "  attributes.GoodForKids                             attributes.GoodForMeal  \\\n",
       "0               b'False'  b\"{'dessert': False, 'latenight': False, 'lunc...   \n",
       "1                b'True'  b\"{'dessert': False, 'latenight': False, 'lunc...   \n",
       "2                    NaN                                                NaN   \n",
       "3                b'True'                                                NaN   \n",
       "4                b'True'  b\"{'dessert': False, 'latenight': False, 'lunc...   \n",
       "\n",
       "                                               hours                   label  \\\n",
       "0  {'Monday': '16:0-2:0', 'Tuesday': '15:0-2:0', ...  american (traditional)   \n",
       "1  {'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...          american (new)   \n",
       "2  {'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...                 mexican   \n",
       "3                                                NaN                 mexican   \n",
       "4                                                NaN                 chinese   \n",
       "\n",
       "                                              review  \\\n",
       "0  So, we stopped here on our way to the Side Que...   \n",
       "1  This is our go-to healthy spot! The food is al...   \n",
       "2  Food court meal at Gerrard Square.  It's been ...   \n",
       "3  Located on Rainbow/Charleston, this small fami...   \n",
       "4  No frills Chinese takeout joint which serves u...   \n",
       "\n",
       "                                                text  \n",
       "0  So stopped way Side Quest street nWe know expe...  \n",
       "1  This go healthy spot The food always fresh del...  \n",
       "2  Food court meal Gerrard Square It since nachos...  \n",
       "3  Located Rainbow Charleston small family owned ...  \n",
       "4  No frills Chinese takeout joint serves best ar...  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.maketrans()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
